# ğŸ”„ AnÃ¡lisis de Concurrencia: MÃºltiples Usuarios Procesando ImÃ¡genes

## ğŸ“‹ Escenario de Prueba

**SituaciÃ³n**: Varios usuarios (ej: 10-20) envÃ­an imÃ¡genes simultÃ¡neamente al backend para:
- Crear reportes con fotos
- Buscar coincidencias por imagen
- Generar embeddings

## âš ï¸ Problemas Identificados

### 1. **LimitaciÃ³n de Concurrencia Parcial**

**Estado Actual**:
```python
# backend/services/embeddings.py
_inference_semaphore = asyncio.Semaphore(2)  # MÃ¡ximo 2 inferencias simultÃ¡neas
```

**Problema**:
- âœ… El semÃ¡foro limita a **2 inferencias simultÃ¡neas** del modelo
- âŒ Pero **NO limita las requests HTTP** que llegan al servidor
- âŒ Los endpoints pueden recibir **muchas mÃ¡s requests** de las que pueden procesar

**Consecuencia**:
- Si 10 usuarios envÃ­an imÃ¡genes al mismo tiempo:
  - 2 se procesan inmediatamente
  - 8 esperan en cola (pueden tardar mucho)
  - Las requests pueden acumularse y causar timeouts

### 2. **Uso Inconsistente de Funciones AsÃ­ncronas**

**Problema Detectado**:

#### âœ… Endpoints que SÃ usan funciÃ³n asÃ­ncrona (respetan semÃ¡foro):
- Ninguno actualmente usa `image_bytes_to_vec_async`

#### âŒ Endpoints que NO usan funciÃ³n asÃ­ncrona (NO respetan semÃ¡foro):
```python
# backend/routers/embeddings_supabase.py
@router.post("/generate")
async def generate_embedding(file: UploadFile = File(...)):
    vec = image_bytes_to_vec(image_bytes)  # âŒ FunciÃ³n SÃNCRONA
    # No respeta el semÃ¡foro de concurrencia
```

```python
# backend/routers/embeddings_supabase.py
@router.post("/search_image")
async def search_image(...):
    qvec = image_bytes_to_vec(await file.read())  # âŒ FunciÃ³n SÃNCRONA
    # No respeta el semÃ¡foro de concurrencia
```

```python
# backend/routers/reports.py
async def generate_and_save_embedding(...):
    vec = image_bytes_to_vec(image_bytes)  # âŒ FunciÃ³n SÃNCRONA
    # No respeta el semÃ¡foro de concurrencia
```

**Consecuencia**:
- El semÃ¡foro **NO se estÃ¡ aplicando** en la mayorÃ­a de endpoints
- MÃºltiples inferencias pueden ejecutarse simultÃ¡neamente
- **Riesgo de sobrecarga de memoria GPU/CPU**
- **Riesgo de crashes** si hay muchas requests

### 3. **Procesamiento SÃ­ncrono en CreaciÃ³n de Reportes**

**Problema**:
```python
# backend/routers/reports.py
@router.post("/")
async def create_report(...):
    # ... crear reporte ...
    
    # âŒ Genera embedding de forma SÃNCRONA antes de retornar
    await generate_and_save_embedding(report_id, first_photo)
    
    return {"report": created_report}  # Usuario espera hasta que termine
```

**Consecuencia**:
- El usuario **espera** hasta que se genere el embedding (puede tardar 2-5 segundos)
- Si hay cola de requests, el tiempo de espera se multiplica
- **Mala experiencia de usuario** (tiempos de respuesta lentos)

### 4. **Falta de Rate Limiting**

**Problema**:
- âŒ No hay lÃ­mite de requests por usuario/IP
- âŒ Un usuario puede enviar muchas imÃ¡genes rÃ¡pidamente
- âŒ Riesgo de **abuso** o **ataques de denegaciÃ³n de servicio**

### 5. **Falta de LÃ­mite de TamaÃ±o de Archivo**

**Problema**:
- âŒ No hay validaciÃ³n del tamaÃ±o mÃ¡ximo de imagen
- âŒ Un usuario puede enviar imÃ¡genes muy grandes (ej: 50MB)
- âŒ Puede causar:
  - **OOM (Out of Memory)** en el servidor
  - **Timeouts** al procesar
  - **Consumo excesivo de ancho de banda**

### 6. **AcumulaciÃ³n de Requests en Cola**

**Problema**:
- Si hay 20 usuarios enviando imÃ¡genes simultÃ¡neamente:
  - Solo 2 se procesan a la vez (si el semÃ¡foro funcionara)
  - 18 esperan en cola
  - Si cada procesamiento tarda 3 segundos:
    - Request #18 esperarÃ¡: 18/2 * 3 = **27 segundos**
  - **Alto riesgo de timeout** (FastAPI default: 60s)

## ğŸ” Comportamiento Actual con MÃºltiples Usuarios

### Escenario: 10 usuarios envÃ­an imÃ¡genes simultÃ¡neamente

```
Tiempo 0s:
  Usuario 1 â†’ Request 1 (procesando)
  Usuario 2 â†’ Request 2 (procesando)
  Usuario 3 â†’ Request 3 (esperando)
  Usuario 4 â†’ Request 4 (esperando)
  ...
  Usuario 10 â†’ Request 10 (esperando)

Tiempo 3s:
  Request 1 âœ… Completado
  Request 2 âœ… Completado
  Request 3 â†’ Inicia procesamiento
  Request 4 â†’ Inicia procesamiento
  Request 5-10 â†’ Siguen esperando

Tiempo 6s:
  Request 3 âœ… Completado
  Request 4 âœ… Completado
  Request 5 â†’ Inicia procesamiento
  Request 6 â†’ Inicia procesamiento
  ...

Tiempo 15s:
  Request 9 â†’ Inicia procesamiento
  Request 10 â†’ Inicia procesamiento

Tiempo 18s:
  Request 9 âœ… Completado
  Request 10 âœ… Completado
```

**Problema Real**: Como el semÃ¡foro NO se estÃ¡ usando correctamente, **TODAS las requests pueden procesarse simultÃ¡neamente**, causando:
- âŒ **Sobrecarga de memoria**
- âŒ **Crashes del servidor**
- âŒ **Timeouts**

## âœ… Soluciones Recomendadas

### 1. **Usar FunciÃ³n AsÃ­ncrona en Todos los Endpoints**

**Cambio necesario**:

```python
# backend/routers/embeddings_supabase.py

# âŒ ANTES
@router.post("/generate")
async def generate_embedding(file: UploadFile = File(...)):
    vec = image_bytes_to_vec(image_bytes)  # SÃ­ncrono

# âœ… DESPUÃ‰S
@router.post("/generate")
async def generate_embedding(file: UploadFile = File(...)):
    vec = await image_bytes_to_vec_async(image_bytes)  # AsÃ­ncrono con semÃ¡foro
```

**Aplicar en**:
- `/embeddings/generate`
- `/embeddings/search_image`
- `/embeddings/index/{report_id}`
- `generate_and_save_embedding()` en `reports.py`

### 2. **Procesar Embeddings en Background Tasks**

**Cambio necesario**:

```python
# backend/routers/reports.py

# âŒ ANTES
@router.post("/")
async def create_report(...):
    # ... crear reporte ...
    await generate_and_save_embedding(report_id, first_photo)  # Bloquea respuesta
    return {"report": created_report}

# âœ… DESPUÃ‰S
@router.post("/")
async def create_report(
    report_data: Dict[str, Any] = Body(...),
    background_tasks: BackgroundTasks
):
    # ... crear reporte ...
    
    # Procesar embedding en background (no bloquea respuesta)
    if photos:
        background_tasks.add_task(
            generate_and_save_embedding,
            report_id,
            first_photo
        )
    
    return {"report": created_report}  # Respuesta inmediata
```

**Beneficios**:
- âœ… Usuario recibe respuesta inmediata
- âœ… Embedding se genera en segundo plano
- âœ… Mejor experiencia de usuario

### 3. **Implementar Rate Limiting**

**SoluciÃ³n**: Usar `slowapi` o `fastapi-limiter`

```python
# backend/main.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Aplicar en endpoints
@router.post("/embeddings/generate")
@limiter.limit("10/minute")  # MÃ¡ximo 10 requests por minuto por IP
async def generate_embedding(...):
    ...
```

### 4. **Validar TamaÃ±o de Archivo**

**SoluciÃ³n**:

```python
# backend/routers/embeddings_supabase.py

MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB

@router.post("/generate")
async def generate_embedding(file: UploadFile = File(...)):
    image_bytes = await file.read()
    
    # Validar tamaÃ±o
    if len(image_bytes) > MAX_IMAGE_SIZE:
        raise HTTPException(
            413,
            f"Imagen demasiado grande. MÃ¡ximo: {MAX_IMAGE_SIZE / 1024 / 1024}MB"
        )
    
    vec = await image_bytes_to_vec_async(image_bytes)
    ...
```

### 5. **Aumentar LÃ­mite del SemÃ¡foro (Opcional)**

**Si el servidor tiene recursos suficientes**:

```python
# backend/services/embeddings.py

# Aumentar de 2 a 4 o mÃ¡s (depende de GPU/RAM disponible)
_inference_semaphore = asyncio.Semaphore(4)  # MÃ¡s inferencias simultÃ¡neas
```

**Consideraciones**:
- âš ï¸ MÃ¡s memoria GPU/RAM necesaria
- âš ï¸ Verificar que el servidor puede soportarlo
- âœ… Mejor throughput (mÃ¡s requests por segundo)

### 6. **Implementar Cola de Procesamiento (SoluciÃ³n Avanzada)**

**Para alta escala**, usar un sistema de colas (Redis + Celery):

```python
# Usar Celery para procesar embeddings en workers separados
from celery import Celery

celery_app = Celery('petalert', broker='redis://localhost:6379')

@celery_app.task
def generate_embedding_task(report_id: str, photo_url: str):
    # Procesar embedding en worker separado
    ...

# En el endpoint
@router.post("/")
async def create_report(...):
    # ... crear reporte ...
    if photos:
        generate_embedding_task.delay(report_id, first_photo)  # Enviar a cola
    return {"report": created_report}
```

**Beneficios**:
- âœ… Escalabilidad horizontal (mÃºltiples workers)
- âœ… No bloquea el servidor principal
- âœ… Mejor manejo de picos de trÃ¡fico

## ğŸ“Š ComparaciÃ³n: Antes vs DespuÃ©s

### Antes (Estado Actual)
- âŒ SemÃ¡foro no se aplica correctamente
- âŒ MÃºltiples inferencias simultÃ¡neas sin control
- âŒ Usuario espera generaciÃ³n de embedding
- âŒ Sin rate limiting
- âŒ Sin validaciÃ³n de tamaÃ±o
- âš ï¸ **Riesgo de crashes con mÃºltiples usuarios**

### DespuÃ©s (Con Soluciones)
- âœ… SemÃ¡foro limita a 2-4 inferencias simultÃ¡neas
- âœ… Control de concurrencia efectivo
- âœ… Usuario recibe respuesta inmediata
- âœ… Rate limiting previene abuso
- âœ… ValidaciÃ³n de tamaÃ±o previene OOM
- âœ… **Sistema estable con mÃºltiples usuarios**

## ğŸ§ª Prueba de Carga Recomendada

**Script de prueba**:

```bash
# Simular 10 usuarios enviando imÃ¡genes simultÃ¡neamente
for i in {1..10}; do
  curl -X POST "http://localhost:8003/embeddings/generate" \
    -F "file=@test_image.jpg" \
    -w "\nTiempo: %{time_total}s\n" &
done
wait
```

**MÃ©tricas a monitorear**:
- Tiempo de respuesta promedio
- Uso de memoria RAM/GPU
- NÃºmero de requests exitosas vs fallidas
- Errores de timeout

## ğŸ¯ Prioridad de ImplementaciÃ³n

1. **ğŸ”´ CRÃTICO**: Usar funciÃ³n asÃ­ncrona en todos los endpoints
2. **ğŸŸ  ALTO**: Procesar embeddings en background tasks
3. **ğŸŸ¡ MEDIO**: Validar tamaÃ±o de archivo
4. **ğŸŸ¢ BAJO**: Implementar rate limiting
5. **ğŸ”µ OPCIONAL**: Sistema de colas (si escala mucho)

---

**ConclusiÃ³n**: El sistema actual **NO estÃ¡ preparado** para manejar mÃºltiples usuarios simultÃ¡neos de forma segura. Se recomienda implementar las soluciones crÃ­ticas antes de producciÃ³n.


