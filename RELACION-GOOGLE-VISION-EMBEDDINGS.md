# ğŸ”— RelaciÃ³n entre Google Vision y Embeddings en PetAlert

Este documento explica cÃ³mo se relacionan y complementan **Google Vision API** y **OpenCLIP Embeddings** en el sistema de bÃºsqueda de mascotas.

## ğŸ“Š Resumen Ejecutivo

**Google Vision** y **Embeddings (OpenCLIP)** son dos tecnologÃ­as **complementarias** que se usan para diferentes propÃ³sitos en el sistema de bÃºsqueda:

| Aspecto | Google Vision | OpenCLIP Embeddings |
|---------|--------------|---------------------|
| **PropÃ³sito** | AnÃ¡lisis semÃ¡ntico | BÃºsqueda por similitud visual |
| **Output** | Etiquetas, colores, especies | Vector de 512 nÃºmeros |
| **MÃ©todo** | DescripciÃ³n textual | RepresentaciÃ³n vectorial completa |
| **Uso Principal** | `/ai-search/` | `/embeddings/search_image` |
| **Almacenamiento** | Columna `labels` (JSONB) | Columna `embedding` (vector(512)) |

## ğŸ” Â¿QuÃ© hace cada uno?

### Google Vision API

**FunciÃ³n:** Analiza la imagen y extrae informaciÃ³n semÃ¡ntica (textual).

**Output:**
```json
{
  "labels": [
    {"label": "Dog", "score": 0.98},
    {"label": "Pet", "score": 0.95},
    {"label": "Golden Retriever", "score": 0.87}
  ],
  "colors": ["#FFD700", "#8B4513", "#FFFFFF"],
  "species": "dog"
}
```

**Se guarda en:** `reports.labels` (JSONB en Supabase)

**Uso:** 
- DetecciÃ³n de especie
- ExtracciÃ³n de caracterÃ­sticas (raza, tamaÃ±o, etc.)
- AnÃ¡lisis de colores
- Filtrado inicial de candidatos

---

### OpenCLIP Embeddings

**FunciÃ³n:** Convierte la imagen en un vector numÃ©rico que captura la similitud visual completa.

**Output:**
```python
# Vector de 512 dimensiones
[0.123, 0.456, -0.789, ..., 0.234]  # 512 nÃºmeros
```

**Se guarda en:** `reports.embedding` (vector(512) en Supabase con pgvector)

**Uso:**
- BÃºsqueda por similitud visual directa
- Encuentra imÃ¡genes "visualmente similares" sin depender de etiquetas
- MÃ¡s preciso para encontrar la misma mascota

---

## ğŸ”„ Flujo Completo del Sistema

### Cuando se crea un reporte:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Usuario crea reporte con foto                   â”‚
â”‚    ğŸ“¸ Foto de la mascota                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Backend procesa la imagen con Google Vision     â”‚
â”‚    - Detecta labels: "dog", "pet", "golden"        â”‚
â”‚    - Detecta colores: ["#FFD700", "#8B4513"]       â”‚
â”‚    - Determina especie: "dog"                      â”‚
â”‚    ğŸ“ Se guarda en: reports.labels (JSONB)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3b. Backend â†’ OpenCLIP (EMBEDDING)                 â”‚
â”‚     - Genera vector[512]                           â”‚
â”‚     - Captura similitud visual completa            â”‚
â”‚     ğŸ“Š Se guarda en: reports.embedding (vector)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Resultado:** Cada reporte tiene:
- âœ… `labels`: AnÃ¡lisis semÃ¡ntico de Google Vision
- âœ… `embedding`: Vector de similitud visual de OpenCLIP

---

## ğŸ” Dos Sistemas de BÃºsqueda Diferentes

### 1. BÃºsqueda con Google Vision (`/ai-search/`)

**CÃ³mo funciona:**
```
Usuario sube foto
    â†“
Google Vision analiza la foto
    â†“
Extrae labels y colores
    â†“
Compara labels con labels almacenados en BD
    â†“
Calcula similitud basada en intersecciÃ³n de labels
    â†“
Retorna resultados ordenados por similitud de labels
```

**CÃ³digo:**
```python
# backend/routers/ai_search.py

# 1. Analizar imagen con Google Vision
vision_client.label_detection(image=image)
# â†’ Obtiene: ["dog", "pet", "golden retriever"]

# 2. Buscar candidatos en BD que tengan labels similares
query = sb.table("reports").select("*")
candidates = query.execute().data

# 3. Calcular similitud comparando labels
visual_score = calculate_visual_similarity(
    {"labels": labels},        # Labels de la foto de bÃºsqueda
    candidate.get("labels", {}) # Labels del candidato en BD
)
# â†’ Similitud basada en etiquetas comunes
```

**Ventajas:**
- âœ… Filtrado rÃ¡pido por especie/raza
- âœ… Interpretable (sabes por quÃ© hay match: "ambos son perros dorados")
- âœ… No requiere embeddings pre-generados

**Limitaciones:**
- âš ï¸ Depende de la calidad de las etiquetas detectadas
- âš ï¸ No captura similitud visual directa
- âš ï¸ Puede perder matches si las etiquetas no coinciden exactamente

---

### 2. BÃºsqueda con Embeddings (`/embeddings/search_image`)

**CÃ³mo funciona:**
```
Usuario sube foto
    â†“
OpenCLIP genera embedding[512] de la foto
    â†“
Busca en BD usando similitud coseno (pgvector)
    â†“
Compara vector de bÃºsqueda con vectores almacenados
    â†“
Retorna resultados ordenados por similitud vectorial
```

**CÃ³digo:**
```python
# backend/routers/embeddings.py

# 1. Generar embedding de la imagen de bÃºsqueda
qvec = image_bytes_to_vec(await file.read())
# â†’ Vector[512]: [0.123, 0.456, -0.789, ...]

# 2. Buscar por similitud usando pgvector
sql = """
    SELECT r.id, (1 - (r.embedding <#> %(qvec)s)) as score_clip
    FROM reports r
    WHERE r.embedding IS NOT NULL
    ORDER BY r.embedding <#> %(qvec)s
    LIMIT 10
"""
# â†’ Similitud coseno directa entre vectores
```

**Ventajas:**
- âœ… Similitud visual precisa (encuentra la misma mascota incluso si las etiquetas difieren)
- âœ… RÃ¡pido con Ã­ndices pgvector
- âœ… No depende de etiquetas textuales

**Limitaciones:**
- âš ï¸ Requiere que todos los reportes tengan embedding generado
- âš ï¸ Menos interpretable (no sabes por quÃ© hay match)

---

## ğŸ¤ Â¿CÃ³mo se Complementan?

### Escenario 1: BÃºsqueda HÃ­brida (Ideal)

```python
# 1. Filtrar candidatos usando Google Vision labels
candidates = filter_by_species(labels)  # Solo perros
candidates = filter_by_breed(labels)    # Solo golden retrievers

# 2. Ordenar por similitud de embeddings
results = sort_by_embedding_similarity(candidates)

# 3. Combinar scores
final_score = (
    label_similarity * 0.3 +    # 30% Google Vision
    embedding_similarity * 0.7   # 70% Embeddings
)
```

**Beneficio:** Combina lo mejor de ambos mundos:
- Filtrado inteligente (Google Vision)
- Similitud visual precisa (Embeddings)

### Escenario 2: Uso Independiente

**Cuando usar Google Vision:**
- BÃºsqueda inicial rÃ¡pida
- Filtrado por especie/raza
- AnÃ¡lisis de caracterÃ­sticas semÃ¡nticas

**Cuando usar Embeddings:**
- BÃºsqueda precisa por similitud visual
- Encontrar la misma mascota con alta confianza
- Cuando las etiquetas no son suficientes

---

## ğŸ“Š Almacenamiento en la Base de Datos

### Tabla `reports` en Supabase:

```sql
CREATE TABLE reports (
    id UUID PRIMARY KEY,
    -- ... otros campos ...
    
    -- Google Vision: AnÃ¡lisis semÃ¡ntico
    labels JSONB,          -- {"tags": ["dog", "pet"], "colors": [...]}
    colors TEXT[],         -- ["#FFD700", "#8B4513"]
    species TEXT,          -- "dog"
    
    -- OpenCLIP: Embedding vectorial
    embedding vector(512),  -- [0.123, 0.456, -0.789, ...]
    
    -- ... otros campos ...
);
```

**Cuando se guarda cada uno:**

| Campo | CuÃ¡ndo se genera | DÃ³nde se genera |
|-------|-----------------|-----------------|
| `labels` | Al crear reporte | Backend con Google Vision |
| `colors` | Al crear reporte | Backend con Google Vision |
| `species` | Al crear reporte | Backend con Google Vision |
| `embedding` | Al indexar reporte (opcional) | Backend con OpenCLIP |

---

## ğŸ”„ Flujo Actual en el Proyecto

### Estado Actual:

1. **Google Vision** (en backend):
   - âœ… Implementado en el backend
   - âœ… Genera labels, colores, especie
   - âœ… Se guarda en `reports.labels`

2. **OpenCLIP Embeddings** (en backend):
   - âœ… Ya estÃ¡ implementado
   - âœ… Genera embeddings vectoriales
   - âœ… Se guarda en `reports.embedding`
   - âš ï¸ Requiere indexaciÃ³n manual o automÃ¡tica

### Endpoints Disponibles:

```python
# 1. BÃºsqueda con Google Vision
POST /ai-search/
# â†’ Usa labels para bÃºsqueda y scoring

# 2. BÃºsqueda con Embeddings
POST /embeddings/search_image
# â†’ Usa embeddings para similitud visual

# 3. Generar embedding para un reporte
POST /embeddings/index/{report_id}
# â†’ Genera y guarda embedding de una imagen
```

---

## ğŸ¯ Recomendaciones

### Para Mejorar la BÃºsqueda:

1. **Generar embeddings automÃ¡ticamente:**
   - Configurar el backend para generar embeddings automÃ¡ticamente al crear reportes

2. **BÃºsqueda hÃ­brida:**
   - Combinar ambos mÃ©todos en un solo endpoint
   - Usar Google Vision para filtrado inicial
   - Usar Embeddings para ranking final

3. **Priorizar embeddings:**
   - Los embeddings capturan similitud visual mÃ¡s precisa
   - Google Vision es Ãºtil para filtrado y metadata
   - Ideal: Google Vision para metadata + Embeddings para bÃºsqueda

---

## ğŸ“ Ejemplo de Uso Combinado

```python
# PseudocÃ³digo de bÃºsqueda hÃ­brida ideal

async def hybrid_search(image_file, user_location):
    # 1. Google Vision: AnÃ¡lisis rÃ¡pido
    labels, colors, species = analyze_with_google_vision(image_file)
    
    # 2. Filtrar candidatos iniciales
    candidates = filter_by_species_and_location(species, user_location)
    
    # 3. Generar embedding de bÃºsqueda
    query_embedding = generate_embedding(image_file)
    
    # 4. Ordenar por similitud de embeddings
    results = rank_by_embedding_similarity(candidates, query_embedding)
    
    # 5. Combinar scores
    for result in results:
        result.final_score = (
            label_similarity(result.labels, labels) * 0.3 +
            embedding_similarity(result.embedding, query_embedding) * 0.7
        )
    
    return sorted(results, key=lambda x: x.final_score, reverse=True)
```

---

## ğŸ‰ ConclusiÃ³n

**Google Vision** y **Embeddings** son tecnologÃ­as **complementarias**:

- **Google Vision** = "Â¿QuÃ© es?" (anÃ¡lisis semÃ¡ntico)
- **Embeddings** = "Â¿Se parece?" (similitud visual)

**Juntos** proporcionan un sistema de bÃºsqueda robusto que combina:
- âœ… Filtrado inteligente (Google Vision)
- âœ… Similitud visual precisa (Embeddings)
- âœ… Metadata rica para mejorar resultados
- âœ… BÃºsqueda rÃ¡pida y escalable


